{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9e9da1a-c112-450d-a843-269781ab8241",
   "metadata": {},
   "source": [
    "# This is a Jupyter Notebook describing how to run PCA using singular value decompostition and sklearn.decomposition PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a944b3a-51f8-402f-8808-bc05e4a7d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import sympy as sp\n",
    "import nmrglue as ng\n",
    "import os\n",
    "import fnmatch\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "sp.init_printing(use_unicode=True)\n",
    "%matplotlib widget\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89bb3c9-2e39-425e-b2da-335dd99a27f6",
   "metadata": {},
   "source": [
    "# First, let's get to some NMR data.\n",
    "## I will find all the ft2 files in directory that holds the 2D 15N HSQC data I want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5d40f6-e21f-4b89-aac3-d609e55b130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in KRAS raw 15N HSQCs\n",
    "os.chdir('/Users/djensen/Documents/KRAS/NMR/')\n",
    "ft2_files = []\n",
    "for file in os.listdir(\"./ft2/Allft2/\"):\n",
    "    if fnmatch.fnmatch(file, '*.ft2'):\n",
    "        ft2_files.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853cbd4d-5ed5-481b-9621-2b8f63b274c1",
   "metadata": {},
   "source": [
    "# The nmr data is about to be compiled into a data table, first I need to define the baseline contour level and the x(1H) and y (15N) limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356054a2-89bb-40d0-9fb4-94b97b6566c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = 130000\n",
    "UH = 11.5\n",
    "LH = 6\n",
    "UN= 135\n",
    "LN = 102.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23067407-1a3d-4b2b-9628-da6a770470de",
   "metadata": {},
   "source": [
    "# Now that the parameters are set, all of the NMR ft2 files are read in one by one.\n",
    "# It will only keep the data from the 1H and 15N Chemical shift limits.\n",
    "# Then it creates two data sets, one with all the data points and another with all the data below the baseline zeroed out.\n",
    "# Next, it generates the new table with the descriptors I want in the file name.\n",
    "# It will also save the shape of the original 2D data and the X/Y limits so that the 2D NMR data can be recreated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77303b94-80ef-41d4-bbed-287090592717",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "for file in ft2_files:\n",
    "    SGU = file.split('_')[0]\n",
    "    mutation = file.split('_')[3]\n",
    "    ligand = file.split('_')[1]\n",
    "    dic, data = ng.pipe.read('./ft2/Allft2/' + file)\n",
    "    uc0 = ng.pipe.make_uc(dic,data,dim=0)\n",
    "    uc1 = ng.pipe.make_uc(dic,data,dim=1)\n",
    "    ndata = data[uc0(str(UN) + ' ppm'):uc0(str(LN) + ' ppm'), uc1(str(UH) + ' ppm'):uc1(str(LH) + ' ppm')]\n",
    "    shape = [ndata.shape[0], ndata.shape[1]]\n",
    "    vec = np.reshape(ndata, newshape = -1)\n",
    "    ndata[(ndata < baseline) & (ndata > -baseline)] = 0\n",
    "    bvec = np.reshape(ndata, newshape = -1)\n",
    "    if counter == 0:\n",
    "        HSQCtable = pd.DataFrame({'SGUid':SGU, 'Mut':mutation, 'Nucleotide':ligand, 'HSQC':[vec], 'UH':[UH], 'LH':[LH], 'UN':[UN], 'LN':[LN], 'Shape':[shape]})\n",
    "        HSQCtable_baseline = pd.DataFrame({'SGUid':SGU, 'Mut':mutation, 'Nucleotide':ligand, 'HSQC':[bvec], 'UH':[UH], 'LH':[LH], 'UN':[UN], 'LN':[LN], 'Shape':[shape]})\n",
    "        counter = counter + 1 \n",
    "    else:\n",
    "        HSQCtable = pd.concat([HSQCtable, pd.DataFrame({'SGUid':SGU, 'Mut':mutation, 'Nucleotide':ligand, 'HSQC':[vec], 'UH':[UH], 'LH':[LH], 'UN':[UN], 'LN':[LN], 'Shape':[shape]})], ignore_index=True)\n",
    "        HSQCtable_baseline = pd.concat([HSQCtable_baseline, pd.DataFrame({'SGUid':SGU, 'Mut':mutation, 'Nucleotide':ligand, 'HSQC':[bvec], 'UH':[UH], 'LH':[LH], 'UN':[UN], 'LN':[LN], 'Shape':[shape]})], ignore_index=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fb2eb8-8e40-4643-ad97-0d4c149bb602",
   "metadata": {},
   "outputs": [],
   "source": [
    "HSQCtable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6108de-4ef8-4dca-bca1-9d409752a19f",
   "metadata": {},
   "source": [
    "### Now, I will generate a numpy data matrix of the HSQC data for the subsequent PCA data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b246358-8036-407c-9bda-53e11bc02428",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for index, row in HSQCtable_baseline.iterrows():\n",
    "    if counter == 0:\n",
    "        data_matrix = np.array(row['HSQC']).astype('float64')\n",
    "        namelist = [row['Mut'] + '_' + row['Nucleotide']]\n",
    "        counter += 1\n",
    "    else:\n",
    "        data_matrix = np.vstack([data_matrix, np.array(row['HSQC']).astype('float64')]) \n",
    "        namelist.append(row['Mut'] + '_' + row['Nucleotide'])\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c797fd73-b00c-40e2-917e-42b35f811d0c",
   "metadata": {},
   "source": [
    "# Now, calculate the mean of each feature ('Pixel') of the NMR spectrum, then subtract it from the data set to 'Mean center' the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2410f0-fd82-48b9-8d94-b62f18696376",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(data_matrix, axis = 0, keepdims=False)\n",
    "centered = data_matrix - mean\n",
    "\n",
    "print('Max value of mean from the uncentered data = ' + str(round(np.mean(data_matrix, axis = 0).max(), 0)))\n",
    "print('Max value of mean from the centered data = ' + str(round(np.mean(centered, axis = 0).max(), 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf17b5e4-489d-4d8a-b1b8-6bb14853666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "centered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d1087b-24da-4cb1-bea9-30b97ed89ce1",
   "metadata": {},
   "source": [
    "# Now we decompose the data matrix using the SVD\n",
    "# The data matrix is 140 X 170K, 140 samples with 170K features.\n",
    "# That means a standard SVD would generate\n",
    "* 140 X 140 <strong>U</strong> matrix (column Eigenvectors)\n",
    "* 140 X 170K <strong>Σ</strong> matrix (square root of Eigenvalues)\n",
    "* 170K X 170K <strong>V<sup>T</sup></strong> matrix (row Eigenvectors)\n",
    "\n",
    "# But the 'Economy SVD' will generate only the non-zero terms in the SVD to save on memory and computation.\n",
    "* 140 X 140 <strong>U</strong> matrix (column Eigenvectors)\n",
    "* 140 X 140 <strong>Σ</strong> matrix (square root of Eigenvalues)\n",
    "* 140 X 170K <strong>V<sup>T</sup></strong> matrix (row Eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8fe39c-90cc-4745-a42e-3d3381a4090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, VT = np.linalg.svd(centered,full_matrices=0)\n",
    "print('Shape of U = ' + str(U.shape[0]) + ' X ' + str(U.shape[1]))\n",
    "print('Shape of Σ = ' + str(S.shape[0]) + ' X ' + str(S.shape[0]))\n",
    "print('Shape of V.T = ' + str(VT.shape[0]) + ' X ' + str(VT.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e99a70-4618-4fbd-99c0-bba7b10046e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('S is the square root of the eigenvector.\\nS^2 will be used to compute percent variance and cumulative variance')\n",
    "S2 = S**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f04d9f-00f0-46c4-baa6-eeaac39ebbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvar = (np.cumsum(S2)/np.sum(S2))[0:len(S2)-1] * 100\n",
    "pvar = ((S2[0:len(S2) -1])/(np.sum(S2[0:len(S2)-1])) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43188ca8-b06e-4821-9e91-0bd19ba216d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize = (12, 6))\n",
    "ax1 = fig1.add_subplot(131)\n",
    "ax1.plot(np.arange(1, len(S2), 1), pvar,'-o',color='k')\n",
    "ax1.set_title('%Variance')\n",
    "ax2 = fig1.add_subplot(132)\n",
    "ax2.semilogy(np.arange(1, len(S2), 1), S[0:len(S) -1],'-o',color='k')\n",
    "ax2.set_title('Singular values')\n",
    "ax3 = fig1.add_subplot(133)\n",
    "ax3.plot(np.arange(1, len(S2), 1), (np.cumsum(S2)/np.sum(S2))[0:len(S2)-1] * 100,'-o',color='k')\n",
    "ax3.set_title('% Cumulative variance')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37a82f3-3650-440f-a0b7-55d9dbd34059",
   "metadata": {},
   "source": [
    "# To create the PCA table, you have to project the data (the mean centered data) onto the principal compontents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68663789-ce6d-46b1-b0f6-49c9cbe31972",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_tab = centered @ VT.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48fb52d-31d8-4ca9-b446-d45652582bc0",
   "metadata": {},
   "source": [
    "# Now that I have the projection vectors (Eigenvectors), I can project the data onto the new dimension and plot Principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f25c75-39be-4657-a4fa-59d19083220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure(figsize=(10,10))\n",
    "ax = fig2.add_subplot(111, projection='3d')\n",
    "\n",
    "for j in range(PCA_tab.shape[0]):\n",
    "    x = PCA_tab[j,0]\n",
    "    y = PCA_tab[j,1]\n",
    "    z = PCA_tab[j,2]\n",
    "    \n",
    "    if namelist[j].split('_')[1] == 'GDP':\n",
    "        ax.scatter(x,y,z,marker='x',color='b',s=50)\n",
    "    else:\n",
    "        ax.scatter(x,y,z,marker='o',color='r',s=50)\n",
    "    n = namelist[j].split('_')[0]\n",
    "    ax.text(x,y,z,s = n)\n",
    "    legend_elements = [Line2D([0], [0], marker='X', color='w', label='GDP',markerfacecolor='b', markersize=15), Line2D([0], [0], marker='o', color='w', label='GPPnP',markerfacecolor='r', markersize=15)]\n",
    "ax.legend(handles=legend_elements, loc='upper left', fontsize = 16)\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "ax.view_init(6,54)\n",
    "#plt.savefig('./PC_loadings_writeout/3D_PC123.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8bd3b5-db68-4884-a11f-1fb79b1bae3f",
   "metadata": {},
   "source": [
    "# Just as a comnparison, I will use the PCA package in scikit learn to calculate principal components and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffd520a-2810-4b43-a7f6-c8962511e192",
   "metadata": {},
   "outputs": [],
   "source": [
    "skpca = PCA(n_components=130)\n",
    "skfitpca = skpca.fit_transform(centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a70e6b-b189-4fa9-ad27-559b47faf53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure(figsize=(10,10))\n",
    "ax = fig2.add_subplot(111, projection='3d')\n",
    "\n",
    "for j in range(skfitpca.shape[0]):\n",
    "    x = skfitpca[j,0]\n",
    "    y = skfitpca[j,1]\n",
    "    z = skfitpca[j,2]\n",
    "    \n",
    "    if namelist[j].split('_')[1] == 'GDP':\n",
    "        ax.scatter(x,y,z,marker='x',color='b',s=50)\n",
    "    else:\n",
    "        ax.scatter(x,y,z,marker='o',color='r',s=50)\n",
    "    n = namelist[j].split('_')[0]\n",
    "    ax.text(x,y,z,s = n)\n",
    "    legend_elements = [Line2D([0], [0], marker='X', color='w', label='GDP',markerfacecolor='b', markersize=15), Line2D([0], [0], marker='o', color='w', label='GPPnP',markerfacecolor='r', markersize=15)]\n",
    "ax.legend(handles=legend_elements, loc='upper left', fontsize = 16)\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "ax.view_init(6,125)\n",
    "#plt.savefig('./PC_loadings_writeout/3D_PC123.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d864c43-6be6-414e-8b73-c0308b6f4c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
